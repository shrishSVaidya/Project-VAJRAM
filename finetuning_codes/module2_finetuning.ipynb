{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "\n",
    "os.environ['HF_HOME'] = '/media/shrish/Data/medgemma_finetune/hf_models/'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/shrish/Data/medgemma_finetune/finetune_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch #\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForImageTextToText,\n",
    "    AutoProcessor, \n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"google/medgemma-1.5-4b-it\" # Replace with your specific MedGemma/Gemma 1.5 path\n",
    "DATASET_PATH = \"medgemma_risk_training_augmented.jsonl\"\n",
    "OUTPUT_DIR = os.path.join(\"models\", \"lora_risk_module2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from medgemma_risk_training_augmented.jsonl...\n"
     ]
    }
   ],
   "source": [
    "print(f\"Loading dataset from {DATASET_PATH}...\")\n",
    "full_dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into Training (90%) and Validation (10%) sets...\n",
      "Training samples: 16056 | Validation samples: 1784\n"
     ]
    }
   ],
   "source": [
    "print(\"Splitting dataset into Training (90%) and Validation (10%) sets...\")\n",
    "# Set a seed so your splits are reproducible across different runs\n",
    "split_dataset = full_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "print(f\"Training samples: {len(train_dataset)} | Validation samples: {len(eval_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing MedGemma 1.5 Processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Processor & Chat Template Formatting ---\n",
    "print(\"Initializing MedGemma 1.5 Processor...\")\n",
    "# MedGemma 1.5 uses a Processor instead of a standalone Tokenizer\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# We extract the underlying tokenizer to pass to the SFTTrainer for text processing\n",
    "tokenizer = processor.tokenizer\n",
    "tokenizer.padding_side = 'right' \n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply MedGemma's specific multimodal chat template to our text array\n",
    "def format_chat_template(example):\n",
    "    example[\"text\"] = processor.apply_chat_template(\n",
    "        example[\"messages\"], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting splits to MedGemma conversational structure...\n"
     ]
    }
   ],
   "source": [
    "print(\"Formatting splits to MedGemma conversational structure...\")\n",
    "# Map the formatting function to both splits independently\n",
    "train_dataset = train_dataset.map(format_chat_template)\n",
    "eval_dataset = eval_dataset.map(format_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base Multimodal Model in 4-bit...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:10<00:00,  5.39s/it]\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 4-Bit Quantization Setup (QLoRA) ---\n",
    "print(\"Loading Base Multimodal Model in 4-bit...\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# CRITICAL: Load using the ImageTextToText class\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model for parameter-efficient fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Injecting LoRA Adapters...\n",
      "trainable params: 32,788,480 || all params: 4,332,867,952 || trainable%: 0.7567\n"
     ]
    }
   ],
   "source": [
    "# --- 4. LoRA Adapter Configuration ---\n",
    "print(\"Injecting LoRA Adapters...\")\n",
    "peft_config = LoraConfig(\n",
    "    r=16, # Rank: Controls the capacity of the adapter\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "new_model = get_peft_model(model, peft_config)\n",
    "new_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,      # Adjust based on your GPU VRAM\n",
    "    per_device_eval_batch_size=2,   \n",
    "    gradient_accumulation_steps=16,      # Simulates a batch size of 8\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"steps\",   \n",
    "    logging_steps=50,\n",
    "    eval_steps=50, \n",
    "    num_train_epochs=3,                 # 3 epochs is standard for medical instruction tuning\n",
    "    max_steps=-1,\n",
    "    fp16=False,\n",
    "    bf16=True,                          # Use bf16 if you have an Ampere GPU (RTX 3000/4000/A100)\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\",                   \n",
    "    run_name=\"medgemma_module2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Supervised Fine-Tuning (SFT) Trainer...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 6. SFT Trainer Initialization ---\n",
    "print(\"Initializing Supervised Fine-Tuning (SFT) Trainer...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, \n",
    "    peft_config=peft_config,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 1, 'bos_token_id': 2, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training... This will take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/shrish/.netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24s004\u001b[0m (\u001b[33mda24s004-iitm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.25.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/media/shrish/Data/medgemma_finetune/wandb/run-20260222_002433-mdm5tyo6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/da24s004-iitm/huggingface/runs/mdm5tyo6' target=\"_blank\">medgemma_module2</a></strong> to <a href='https://wandb.ai/da24s004-iitm/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/da24s004-iitm/huggingface' target=\"_blank\">https://wandb.ai/da24s004-iitm/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/da24s004-iitm/huggingface/runs/mdm5tyo6' target=\"_blank\">https://wandb.ai/da24s004-iitm/huggingface/runs/mdm5tyo6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/shrish/Data/medgemma_finetune/finetune_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/media/shrish/Data/medgemma_finetune/finetune_env/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1506' max='1506' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1506/1506 28:46:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.518400</td>\n",
       "      <td>0.396847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.313000</td>\n",
       "      <td>0.354317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.308800</td>\n",
       "      <td>0.347138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.307800</td>\n",
       "      <td>0.336562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.330142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.299000</td>\n",
       "      <td>0.316288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.304725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.287800</td>\n",
       "      <td>0.305818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.287400</td>\n",
       "      <td>0.296518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.286800</td>\n",
       "      <td>0.296490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.277900</td>\n",
       "      <td>0.286845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.264200</td>\n",
       "      <td>0.281682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.258500</td>\n",
       "      <td>0.275208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.257900</td>\n",
       "      <td>0.270953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.265408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.257555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.246400</td>\n",
       "      <td>0.248673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.238200</td>\n",
       "      <td>0.239095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.226600</td>\n",
       "      <td>0.232261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.224200</td>\n",
       "      <td>0.227423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.224392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.196100</td>\n",
       "      <td>0.221149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.199600</td>\n",
       "      <td>0.213304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.210423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.193100</td>\n",
       "      <td>0.207562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.187800</td>\n",
       "      <td>0.205987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.188800</td>\n",
       "      <td>0.205333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.187400</td>\n",
       "      <td>0.205014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.186800</td>\n",
       "      <td>0.205057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.187200</td>\n",
       "      <td>0.205088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/shrish/Data/medgemma_finetune/finetune_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/media/shrish/Data/medgemma_finetune/finetune_env/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/media/shrish/Data/medgemma_finetune/finetune_env/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/media/shrish/Data/medgemma_finetune/finetune_env/lib/python3.12/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1506, training_loss=0.25365220677171885, metrics={'train_runtime': 103679.9449, 'train_samples_per_second': 0.465, 'train_steps_per_second': 0.015, 'total_flos': 1.9307331622164768e+17, 'train_loss': 0.25365220677171885})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 7. Execute Training ---\n",
    "print(\"Beginning training... This will take some time.\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Saving adapter to models/lora_risk_module2...\n",
      "‚úÖ Module 2 Adapter successfully compiled and saved.\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Save the Final LoRA Adapter ---\n",
    "print(f\"Training complete. Saving adapter to {OUTPUT_DIR}...\")\n",
    "trainer.model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"‚úÖ Module 2 Adapter successfully compiled and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "üöÄ Starting Final Evaluation on Validation Set...\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma3ForConditionalGeneration(\n",
       "      (model): Gemma3Model(\n",
       "        (vision_tower): SiglipVisionModel(\n",
       "          (vision_model): SiglipVisionTransformer(\n",
       "            (embeddings): SiglipVisionEmbeddings(\n",
       "              (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "              (position_embedding): Embedding(4096, 1152)\n",
       "            )\n",
       "            (encoder): SiglipEncoder(\n",
       "              (layers): ModuleList(\n",
       "                (0-26): 27 x SiglipEncoderLayer(\n",
       "                  (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                  (self_attn): SiglipAttention(\n",
       "                    (k_proj): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1152, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=1152, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (v_proj): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1152, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=1152, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (q_proj): lora.Linear4bit(\n",
       "                      (base_layer): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Dropout(p=0.05, inplace=False)\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1152, out_features=16, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=16, out_features=1152, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
       "                  )\n",
       "                  (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "                  (mlp): SiglipMLP(\n",
       "                    (activation_fn): GELUTanh()\n",
       "                    (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
       "                    (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (multi_modal_projector): Gemma3MultiModalProjector(\n",
       "          (mm_soft_emb_norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "          (avg_pool): AvgPool2d(kernel_size=4, stride=4, padding=0)\n",
       "        )\n",
       "        (language_model): Gemma3TextModel(\n",
       "          (embed_tokens): Gemma3TextScaledWordEmbedding(262208, 2560, padding_idx=0)\n",
       "          (layers): ModuleList(\n",
       "            (0-33): 34 x Gemma3DecoderLayer(\n",
       "              (self_attn): Gemma3Attention(\n",
       "                (q_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=2048, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (v_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2048, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "              )\n",
       "              (mlp): Gemma3MLP(\n",
       "                (gate_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (up_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=2560, out_features=10240, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=2560, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=10240, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (down_proj): lora.Linear4bit(\n",
       "                  (base_layer): Linear4bit(in_features=10240, out_features=2560, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=10240, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=16, out_features=2560, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (act_fn): GELUTanh()\n",
       "              )\n",
       "              (input_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_attention_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (pre_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "              (post_feedforward_layernorm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "            )\n",
       "          )\n",
       "          (norm): Gemma3RMSNorm((2560,), eps=1e-06)\n",
       "          (rotary_emb): Gemma3RotaryEmbedding()\n",
       "          (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2560, out_features=262208, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import wandb\n",
    "\n",
    "# --- 9. Post-Training Evaluation on Validation Set ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üöÄ Starting Final Evaluation on Validation Set...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 1. Put the newly trained model in evaluation mode\n",
    "trainer.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Helper function to parse the specific Risk Category from the text\n",
    "def extract_risk_level(text):\n",
    "    text_lower = text.lower()\n",
    "    if \"critical risk\" in text_lower: \n",
    "        return \"Critical Risk\"\n",
    "    elif \"moderate/high risk\" in text_lower or \"moderate risk\" in text_lower: \n",
    "        return \"Moderate/High Risk\"\n",
    "    elif \"high risk\" in text_lower: \n",
    "        return \"High Risk\"\n",
    "    elif \"standard/low risk\" in text_lower or \"low risk\" in text_lower: \n",
    "        return \"Standard/Low Risk\"\n",
    "    return \"Unknown/Invalid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating Module 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1784/1784 [2:25:57<00:00,  4.91s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference complete! Calculating metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Iterate through the validation dataset\n",
    "for example in tqdm(eval_dataset, desc=\"Evaluating Module 2\"):\n",
    "    # The dataset uses a \"messages\" schema\n",
    "    user_msg = [msg for msg in example[\"messages\"] if msg[\"role\"] == \"user\"]\n",
    "    true_target = [msg for msg in example[\"messages\"] if msg[\"role\"] == \"assistant\"][0][\"content\"]\n",
    "    \n",
    "    # Format the prompt using the chat template\n",
    "    prompt = processor.apply_chat_template(\n",
    "        user_msg, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize and move to GPU\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\", padding=True).to(trainer.model.device)\n",
    "    \n",
    "    inputs.pop(\"token_type_ids\", None)\n",
    "    inputs.pop(\"pixel_values\", None)\n",
    "\n",
    "    # Generate the response\n",
    "    with torch.no_grad():\n",
    "        outputs = trainer.model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=150, # Enough tokens to capture the risk score and the reasoning\n",
    "            do_sample=False,    # Greedy decoding for consistent clinical evaluation\n",
    "            pad_token_id=processor.tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "    # Decode only the newly generated tokens\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "    generated_tokens = outputs[0, input_length:]\n",
    "    pred_text = processor.decode(generated_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    # 3. Parse the risk classification\n",
    "    true_risk = extract_risk_level(true_target)\n",
    "    pred_risk = extract_risk_level(pred_text)\n",
    "    \n",
    "    is_correct = (true_risk == pred_risk)\n",
    "    \n",
    "    results.append({\n",
    "        \"User_Prompt\": user_msg[0][\"content\"],\n",
    "        \"Ground_Truth_Text\": true_target,\n",
    "        \"Model_Prediction_Text\": pred_text,\n",
    "        \"True_Label\": true_risk,\n",
    "        \"Predicted_Label\": pred_risk,\n",
    "        \"Correct\": is_correct\n",
    "    })\n",
    "\n",
    "print(\"\\nInference complete! Calculating metrics...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üìä MODULE 2 METRICS (Risk Stratification)\n",
      "========================================\n",
      "Accuracy:  0.9989\n",
      "Weighted Precision: 0.9989\n",
      "Weighted Recall:    0.9989\n",
      "Weighted F1 Score:  0.9989\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 10. Calculate Metrics & Categorize Samples ---\n",
    "\n",
    "y_true = [r[\"True_Label\"] for r in results]\n",
    "y_pred = [r[\"Predicted_Label\"] for r in results]\n",
    "\n",
    "# Calculate Multi-Class Metrics (Using 'weighted' average since class distribution may be imbalanced)\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=\"weighted\", zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"üìä MODULE 2 METRICS (Risk Stratification)\")\n",
    "print(\"=\"*40)\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Weighted Precision: {precision:.4f}\")\n",
    "print(f\"Weighted Recall:    {recall:.4f}\")\n",
    "print(f\"Weighted F1 Score:  {f1:.4f}\")\n",
    "print(\"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 5 RANDOM VALIDATION SAMPLES ---\n",
      "\n",
      "Sample #1 ‚úÖ\n",
      "  True Risk Level: Standard/Low Risk | Predicted: Standard/Low Risk\n",
      "User Prompt: Assess the Multiple Myeloma risk profile for this 64-year-old Male patient:\n",
      "- Patient Reported Symptoms: Patient presented with no specific symptoms noted in the provided snippet.\n",
      "- CRAB Panel -> Creatinine: 0.80 mg/dL, Calcium: 7.75 mg/dL, Hemoglobin: 11.70 g/dL\n",
      "- Tumor/Staging Panel -> Albumin: Not tested, Beta-2 Microglobulin: Not tested, LDH: Not tested, M-Spike (SPEP): Not tested, FLC Ratio: Not tested\n",
      "  Ground Truth: Standard/Low Risk based on currently available data. No overt CRAB criteria or tumor markers are met. Continue routine clinical monitoring based on presenting symptoms.\n",
      "  Model Output: Standard/Low Risk based on currently available data. No overt CRAB criteria or tumor markers are met. Continue routine clinical monitoring based on presenting symptoms.\n",
      "\n",
      "Sample #2 ‚úÖ\n",
      "  True Risk Level: Standard/Low Risk | Predicted: Standard/Low Risk\n",
      "User Prompt: Assess the Multiple Myeloma risk profile for this 64-year-old Male patient:\n",
      "- Patient Reported Symptoms: Please provide the note snippet for me to synthesize the Chief Complaint and admission symptoms.\n",
      "- CRAB Panel -> Creatinine: 0.90 mg/dL, Calcium: Not tested, Hemoglobin: 12.90 g/dL\n",
      "- Tumor/Staging Panel -> Albumin: Not tested, Beta-2 Microglobulin: Not tested, LDH: Not tested, M-Spike (SPEP): Not tested, FLC Ratio: Not tested\n",
      "  Ground Truth: Standard/Low Risk based on currently available data. No overt CRAB criteria or tumor markers are met. Continue routine clinical monitoring based on presenting symptoms.\n",
      "  Model Output: Standard/Low Risk based on currently available data. No overt CRAB criteria or tumor markers are met. Continue routine clinical monitoring based on presenting symptoms.\n",
      "\n",
      "Sample #3 ‚úÖ\n",
      "  True Risk Level: High Risk | Predicted: High Risk\n",
      "User Prompt: Assess the Multiple Myeloma risk profile for this 88-year-old Male patient:\n",
      "- Patient Reported Symptoms: Patient presented with bleeding after he pulled out his GJ-tube. He was recently admitted several times for issues including respiratory distress, volume overload, and difficulty with swallowing.\n",
      "- CRAB Panel -> Creatinine: 1.40 mg/dL, Calcium: 9.30 mg/dL, Hemoglobin: 8.42 g/dL\n",
      "- Tumor/Staging Panel -> Albumin: Not tested, Beta-2 Microglobulin: Not tested, LDH: Not tested, M-Spike (SPEP): Not tested, FLC Ratio: Not tested\n",
      "  Ground Truth: High Risk: Suspected End-Organ Damage. Patient exhibits CRAB criteria (Anemia). However, specialized tumor markers (SPEP/FLC) are missing. Given the clinical presentation, an urgent hematology referral for a full myeloma workup is required.\n",
      "  Model Output: High Risk: Suspected End-Organ Damage. Patient exhibits CRAB criteria (Anemia). However, specialized tumor markers (SPEP/FLC) are missing. Given the clinical presentation, an urgent hematology referral for a full myeloma workup is required.\n",
      "\n",
      "Sample #4 ‚úÖ\n",
      "  True Risk Level: Standard/Low Risk | Predicted: Standard/Low Risk\n",
      "User Prompt: Assess the Multiple Myeloma risk profile for this 58-year-old Male patient:\n",
      "- Patient Reported Symptoms: Patient presented with worsening infectious symptoms over the weekend, including hoarseness, chills, shaking, diffuse whole body pain, fever (101.0¬∞F), and episodes of cough associated with dark blood. Additionally, he experienced an episode where his legs collapsed at home and needed help to rise.\n",
      "- CRAB Panel -> Creatinine: Not tested, Calcium: 9.23 mg/dL, Hemoglobin: Not tested\n",
      "- Tumor/Staging Panel -> Albumin: Not tested, Beta-2 Microglobulin: Not tested, LDH: Not tested, M-Spike (SPEP): Not tested, FLC Ratio: Not tested\n",
      "  Ground Truth: Standard/Low Risk based on currently available data. No overt CRAB criteria or tumor markers are met. Continue routine clinical monitoring based on presenting symptoms.\n",
      "  Model Output: Standard/Low Risk based on currently available data. No overt CRAB criteria or tumor markers are met. Continue routine clinical monitoring based on presenting symptoms.\n",
      "\n",
      "Sample #5 ‚úÖ\n",
      "  True Risk Level: Critical Risk | Predicted: Critical Risk\n",
      "User Prompt: Assess the Multiple Myeloma risk profile for this 67-year-old Female patient:\n",
      "- Patient Reported Symptoms: No acute behavioral complaints documented.\n",
      "- CRAB Panel -> Creatinine: 1.10 mg/dL, Calcium: 9.81 mg/dL, Hemoglobin: 7.35 g/dL\n",
      "- Tumor/Staging Panel -> Albumin: 2.74 g/dL, Beta-2 Microglobulin: Not tested, LDH: 861.00 U/L, M-Spike (SPEP): Not tested, FLC Ratio: 0.01\n",
      "  Ground Truth: Critical Risk: Active Multiple Myeloma with End-Organ Damage. Patient exhibits CRAB criteria (Anemia) and high tumor burden markers (Abnormal FLC Ratio). Immediate intervention required.\n",
      "  Model Output: Critical Risk: Active Multiple Myeloma with End-Organ Damage. Patient exhibits CRAB criteria (Anemia) and high tumor burden markers (Abnormal FLC Ratio). Immediate intervention required.\n"
     ]
    }
   ],
   "source": [
    "# Select 5 random samples\n",
    "random_5 = random.sample(results, min(5, len(results)))\n",
    "\n",
    "print(\"--- 5 RANDOM VALIDATION SAMPLES ---\")\n",
    "for i, s in enumerate(random_5):\n",
    "    status = \"‚úÖ\" if s['Correct'] else \"‚ùå\"\n",
    "    print(f\"\\nSample #{i+1} {status}\")\n",
    "    print(f\"  True Risk Level: {s['True_Label']} | Predicted: {s['Predicted_Label']}\")\n",
    "    print(f\"User Prompt: {s['User_Prompt']}\")\n",
    "    print(f\"  Ground Truth: {s['Ground_Truth_Text']}\") # Truncated for terminal readability\n",
    "    print(f\"  Model Output: {s['Model_Prediction_Text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    }
   ],
   "source": [
    "# --- 11. Log Everything to Weights & Biases ---\n",
    "\n",
    "# Ensure wandb is tracking (Trainer initializes it automatically, but we ensure it's active)\n",
    "if wandb.run is None:\n",
    "    wandb.init(project=\"huggingface\", name=\"medgemma_module2_eval\")\n",
    "\n",
    "# Log numeric metrics\n",
    "wandb.log({\n",
    "    \"eval/accuracy\": accuracy,\n",
    "    \"eval/precision\": precision,\n",
    "    \"eval/recall\": recall,\n",
    "    \"eval/f1_score\": f1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully logged all metrics and triage samples to Weights & Biases!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Build a WandB Table to log the text samples cleanly\n",
    "columns = [\"True Risk Level\", \"Predicted Risk Level\", \"Correct?\", \"Ground Truth Reasoning\", \"Model Predicted Reasoning\", \"Input Clinical Profile\"]\n",
    "eval_table = wandb.Table(columns=columns)\n",
    "\n",
    "# Add our random 5 samples to the table\n",
    "for s in random_5:\n",
    "    eval_table.add_data(\n",
    "        s[\"True_Label\"], \n",
    "        s[\"Predicted_Label\"], \n",
    "        s[\"Correct\"], \n",
    "        s[\"Ground_Truth_Text\"], \n",
    "        s[\"Model_Prediction_Text\"],\n",
    "        s[\"User_Prompt\"]\n",
    "    )\n",
    "\n",
    "# Log the table to your wandb dashboard\n",
    "wandb.log({\"Module_2_Validation_Samples\": eval_table})\n",
    "\n",
    "print(\"‚úÖ Successfully logged all metrics and triage samples to Weights & Biases!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finetune_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
